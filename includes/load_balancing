<h1 id='gunrock-39-s-load-balancing-policies-load_balancing'>Gunrock&#39;s Load Balancing Policies              {#load_balancing}</h1>
<p>While graph computation exhibits ample fine-grained parallelism, that parallelism is highly irregular. If we parallelize computation over vertices, for instance, neighboring vertices may have very different amounts of work. On a SIMD machine like a GPU, addressing this load imbalance is critical for performance.</p>

<p>The most significant low-level reason Gunrock demonstrates good performance on graph computation is its load-balancing implementation. Gunrock incorporates several different high-performance load-balancing policies. What are they and how do they work?</p>

<p>In this discussion we specifically consider an <em>advance</em> operation on an input frontier of vertices that outputs a frontier of neighboring vertices. Informally, this operation asks &quot;what are the neighboring vertices to the current frontier&quot;. Gunrock&#39;s <em>advance</em> also supports input and/or output frontiers of edges, but in this document we focus on input and output frontiers of vertices.</p>
<h2 id='gunrock-39-s-advance-operator'>Gunrock&#39;s Advance operator</h2>
<p>The entry point to an <em>advance</em> operation is the <code>gunrock::oprtr::Advance</code> call. For instance, SSSP has an <code>SSSPIterationLoop</code> struct, which contains a <code>Core</code> member function, which calls <code>oprtr::Advance</code>. <code>Advance</code> is templated by the operator type, which specifies the input and output types (vertices or edges). SSSP uses <code>oprtr::OprtrType_V2V</code>. <code>Advance</code> is also templated by the datatypes of its 6 arguments:</p>

<ul>
<li>The input graph</li>
<li>The input frontier</li>
<li>The output frontier</li>
<li>The operator parameters</li>
<li>The advance operation</li>
<li>The filter operation</li>
</ul>

<p>The first three are straightforward. The second three need a little explanation.</p>
<h3 id='operator-parameters'>Operator parameters</h3>
<p>The operator parameters (<code>oprtr_parameters</code>) hold configuration information for a variety of user-settable parameters. The important parameter for this discussion is <code>advance_mode</code>, which specifies the load-balancing policy as a string, and is then used by the operator to choose its mode.</p>
<h3 id='advance-operation'>Advance operation</h3>
<p>The advance operation is a function that runs on a potential vertex in the output frontier. It returns true or false, and only if it returns true is that potential vertex placed in the output frontier.</p>
<h3 id='filter-operation'>Filter operation</h3>
<p>The filter operation has identical behavior to the advance operation. It would (presumably) be used when Gunrock fuses an advance followed by a filter into a single operator.</p>
<h2 id='static-workload-mapping-policy-threadexpand'>Static Workload Mapping Policy (<code>ThreadExpand</code>)</h2>
<p>The simplest policy is to not attempt any load-balance at all. We assign each vertex in the input frontier to its own thread. Each thread then loads its assigned input vertex&#39;s neighbor list and serially iterates through this neighbor list. Gunrock calls this policy <code>ThreadExpand</code>.</p>

<p>The obvious disadvantage of this policy is that significant imbalance in work among neighboring vertices leads to poor performance. However, it has minimal load-balancing overhead, and if the input graph has a fairly uniform distribution of edges per vertex, it performs fairly well.</p>

<p>In Gunrock, we can enable <code>ThreadExpand</code> by configuring the next strategy, as we will describe below.</p>

<p>yzh: &quot;<code>ThreadExpand</code>, <code>WarpExpand</code> and <code>CtaExpand</code> together form both <code>TWC_FORWARD</code> and <code>TWC_BACKWARD</code> mode. You will see <code>ThreadExpand</code> appear in both <code>edge_map_forward/cta.cuh</code> and <code>edge_map_backward/cta.cuh</code>. <code>ThreadExpand()</code> maps to Merrill&#39;s <code>ExpandByScan()</code>. It uses each thread to put each node&#39;s neighbor list node ids to shared memory (https://github.com/gunrock/gunrock/blob/master/gunrock/oprtr/edge_map_forward/cta.cuh#L900). Then use one thread to expand several neighbor nodes (not necessarily from one input node) in a for loop: https://github.com/gunrock/gunrock/blob/master/gunrock/oprtr/edge_map_forward/cta.cuh#L907 &quot;</p>

<p>there is no way to tell Gunrock &quot;use <code>ThreadExpand</code> only&quot; then.</p>

<p>yzh: &quot;Actually there is a way. First you choose <code>TWC_FORWARD</code> mode, then here: https://github.com/gunrock/gunrock/blob/master/gunrock/app/bfs/bfs_enactor.cuh#L2020 You set <code>WARP_GATHER_THRESHOLD</code> to be std::numeric_limits<int>::max(), then none of the neighbor list will be considered as big enough to use <code>WarpExpand</code> or <code>CTAExpand</code>: https://github.com/gunrock/gunrock/blob/master/gunrock/oprtr/edge_map_forward/cta.cuh#L248 By setting <code>CTA_GATHER_THRESHOLD</code> to be the same or less than <code>WARP_GATHER_THRESHOLD</code>: , you can disable <code>WarpExpand()</code>: https://github.com/gunrock/gunrock/blob/master/gunrock/oprtr/edge_map_forward/cta.cuh#L501 By setting <code>CTA_GATHER_THRESHOLD</code> to be larger than the max possible neighbor list length, you can disable <code>CtaExpand()</code>: https://github.com/gunrock/gunrock/blob/master/gunrock/oprtr/edge_map_forward/cta.cuh#L390 &quot;</p>

<p>yzh: &quot;<code>work_limits.elements = grains_per_cta &lt;&lt; LOG_SCHEDULE_GRANULARITY;</code> is the number of input elements processed per block. This number is further divided into tiles. Every ProcessTile() call will push the offset: <code>work_limits.offset += KernelPolicy::TILE_ELEMENTS</code>. So grain is another dimension of assigning input elements to CTAs. tile_size is how many input elements get processed together in a block. In our current settings, <code>TILE_ELEMENTS</code> is 256, meaning one <code>ProcessTile()</code> will expand 256 neighbor lists from input queue in a block. <code>grain_per_cta</code> is related to input queue length and block number for launching config.&quot;</p>
<h2 id='dynamic-workload-mapping-policy-twc'>Dynamic Workload Mapping Policy (<code>TWC</code>)</h2>
<p><code>ThreadExpand</code>&#39;s disadvantage is its inability to load-balance across vertices with varying numbers of neighbors. Merrill et al.&#39;s <a href="http://research.nvidia.com/publication/scalable-gpu-graph-traversal">seminal BFS implementation</a> addresses this issue with its <code>TWC</code> (&quot;thread-warp-CTA&quot;) policy. Gunrock&#39;s implementation is similar to Merrill&#39;s.</p>

<p>TWC groups neighbor lists into three categories based on their size, then individually processes each category with a strategy targeted directly at that size. Our three sizes are 1) lists larger than a block, 2) lists larger than a warp (32 threads) but smaller than a block, and 3) lists smaller than a warp. These correspond to Gunrock&#39;s <code>CtaExpand()</code>, <code>WarpExpand()</code>, and <code>ThreadExpand()</code>. The programmer can manually set the thresholds for switch-points between these three strategies: <code>CTA_GATHER_THRESHOLD</code> is the breakpoint between the first two; <code>WARP_GATHER_THRESHOLD</code> is the breakpoint between the second two. If both are set to very large values, <code>TWC</code> devolves to <code>ThreadExpand</code>.</p>

<p>We begin by assigning a subset of the frontier to a block. Within that block, each thread owns one node.  The threads that own nodes with large lists arbitrate for control of the entire block. All the threads in the block then cooperatively process the neighbor list of the winnerâ€™s node.  This procedure continues until all nodes with large lists have been processed. Next, all threads in each warp begin a similar procedure to process all the nodes with medium-sized lists. Finally, the remaining nodes are processed using <code>ThreadExpand</code>. As Merrill et al. noted in their paper, this strategy can guarantee high utilization of resources and limit various types of load imbalance such as SIMD lane underutilization (by using per-thread mapping), intra-thread imbalance (by using warp-wise mapping), and intra-warp imbalance (by using block-wise mapping).</p>

<p>In Gunrock, there are two <code>TWC</code> policies, <code>TWC_FORWARD</code> (calls <code>gunrock::oprtr::edge_map_forward</code>) and <code>TWC_BACKWARD</code> (calls <code>gunrock::oprtr::edge_map_backward</code>).</p>
<h3 id='edge_map_forward-oprtr-edge_map_forward'><code>edge_map_forward</code>   (<code>oprtr/edge_map_forward</code>)</h3>
<ul>
<li>One thread in each block:

<ul>
<li>Computes the problem size</li>
<li>Initializes shared memory to store work</li>
</ul></li>
<li>Then call <code>Sweep::Invoke</code>, which:

<ul>
<li>Determines the chunk of work (&quot;tile&quot;) this thread block will tackle (<code>GetCtaWorkLimits</code>)

<ul>
<li>This is in <code>util/cta_work_distribution.cuh</code>. It basically divides the work into &quot;grains&quot; and assigns grains to CTAs (each CTA gets a contiguous number of grains, which together are a tile).</li>
</ul></li>
<li>Processes the tile (<code>cta.ProcessTile</code> in <code>cta.cuh</code>)

<ul>
<li>Load the tile (<code>LoadValid</code>)</li>
<li><code>Inspect</code>: &quot;Inspect the neighbor list size of each node in the frontier, prepare for neighbor list expansion&quot;. This starts with the starting vertex ID for the tile (?) and then loads the neighbor row range from the CSR row-offset vector (?)

<ul>
<li>This is hard-coded to CSR</li>
</ul></li>
<li><code>CooperativeSoaTileScan</code></li>
</ul></li>
<li>Cleans up the last partial tile</li>
</ul></li>
</ul>
