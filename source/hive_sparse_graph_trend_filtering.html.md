---
title: Sparse Fused Lasso (HIVE)

toc_footers:
  - <a href='https://github.com/gunrock/gunrock'>Gunrock&colon; GPU Graph Analytics</a>
  - Gunrock &copy; 2018 The Regents of the University of California.

search: true

full_length: true
---

# Sparse Fused Lasso

Given a graph where each vertex on the graph has a weight, _sparse fused lasso (SFL)_, also named _sparse graph trend filter (GTF)_, tries to learn a new weight for each vertex that is (1) sparse (most vertices have weight 0), (2) close to the original weight in the l2 norm, and (3) close to its neighbors' weight(s) in the l1 norm. This algorithm is usually used in main trend filtering (denoising). The loss function is `0.5 * sum((y' - y)^2) + lambda1 * sum(abs(y_i' - y_j')) + lambda2 * sum(abs(yi'))`, where `y` is the input weight (observation) for each vertex and `y'` (fitted weight) is the new weight for each vertex. For example, an image (grid graph) with noisy pixels can be filtered with this algorithm to get a new image without the noisy pixels, which are "smoothed out" by its neighbors.
<https://arxiv.org/abs/1410.7690>

## Summary of Results

The SFL problem is mainly divided into two parts, computing residual graphs from maxflow and renormalizing the weights of the vertices. Maxflow is parallelizable with the push-relabel algorithm, so we adopt this algorithm in Gunrock's implementation. Moreover, each vertex has its own work to compute which communities it belongs to, and normalize the weights with other vertices in the same community. This renormalization requires global synchronization. SFL iterates by calling maxflow and renormalization several times before it converges. We notice that the overall runtime is mostly spent in maxflow, and thus improving the maxflow implementation will bring substantial speedup in the SFL.

Because of the current state of our maxflow implementation, we notice a 30x slowdown of Gunrock's GPU SFL with respect to the benchmark implementation. Some limitations lead to this slowdown, including (1) substantial extra memory transfers between the CPU and GPU because of maxflow's relabeling heuristics (2) maxflow's preprocessing poor iteration boundary of min-cut on GPU, and (3) a serial implementation of renormalization. These problems could be addressed  in a future version of SFL.

## Summary of Gunrock Implementation

The input graph is specified in two files. The first file contains the original vertices' weights and the second file contains the directed graph connectivity without weights (edge pairs only). These two files and a edge_regularization_strength (`lambda1`) of the directed graph edge weights are the input to preprocessing. Two extra nodes, source and sink, are added to the original graph as well. They serve as two "labels" of different segments on the graph. This results in a graph where edges that connect to the source or sink have edge-weights as in the `vertices' weights` file, and the other edges are assigned an edge weight of `lambda1`.

The Gunrock implementation of this application has two parts. The first part is the maxflow algorithm. We choose a push-relabel maxflow formulation, which is well-suited for parallelization on a GPU with Gunrock. Computing a valid maxflow also implies a solution to the mincut problem, which is a segmentation of a graph into two pieces where the division is across a set of edges with the minimum possible weights. The output of this maxflow algorithm is (1) a residual graph where each edge weight is computed as `capacity - edge_flow`, and (2) a Boolean array that marks which nodes are reachable from the source after the mincut.

The second part is a renormalization of the residual graph and clustering based on reachability of the vertex. The renormalization is a process where (1) averages of the new weights of vertices that are grouped together as communities are computed, and (2) the new weights then subtract their own community averages. After the renormalization is done, this renormalized residual graph is passed into the maxflow again. Several iterations of maxflow then renormalization are needed before the new weights of different communities converge because vertices can be reassigned to different communities. In each of the SFL iterations, two non-overlapped subgraphs will be generated by maxflow/mincut, and thus the big communities in the last SFL iteration will have split into small communities. The vertices in a specific community will have the same new weights assigned to them.

The outputs will be the normalized values assigned to each vertex.

Lastly, these values will be passed into a soft-threshold function with `lambda2` to achieve the sparse representation by dropping the small absolute values. More specifically, the new weight will be subtracted by `lambda2` if the new weight is positive and larger than `lambda2`, or added by `lambda2` if the new weight is negative and smaller than `-lambda2`. If the new weight is in between `-lambda2` to `lambda2`, then the new weights will be 0.

Pseudocode for the core SFL algorithm is as follows (simplified version):

```
Load the graph and normalize edge weights

for iteration till converge:

    # First part: Maxflow
    Call maxflow data preprocessing
    Call maxflow and return boolean reachability array and residual graph


    # Second part: Reset available community
    for comm till num_comms
        community_weights[comm] = 0
        community_sizes  [comm] = 0
        next_communities [comm] = 0

    # Second part (1): Accumulate the weights and count the number of vertices belong to the communities
    for vertex in the graph
        if vertex accessible from the source
            comm = next_communities[curr_communities[vertex]];
                if comm == 0
                    update comm
            community_sizes[comm]++
            community_weights[comm] += weight between source and this current vertex
        else
            community_weights[comm] -= weight between vertex and this sink
            community_sizes [comm] ++

    # Second part (2): Normalize community
    for comm in num_communities
        community_weights[comm] /= community_sizes[comm]
        community_accus [comm] += community_weights[comm]

    # Update the residual graph
    for vertex in the graph
        comm = curr_communities[vertex]
        if vertex is reachable from the source
            edge[source->vertex] -= community_weights[comm]
            if edge[source->vertex] < 0
                swap(-edge[source->vertex], edge[vertex->sink])
        else
            edge[vertex->sink] += community_weights[comm]
            if edge[vertex->sink] < 0
                swap(edge[source->vertex], -edge[vertex->sink])

# Part 3
Sparsify community_accus by lambda2

output: community_accus
```


## How To Run This Application on DARPA's DGX-1

### Prereqs/input

CUDA should have been installed; `$PATH` and `$LD_LIBRARY_PATH` should have been
set correctly to use CUDA. The current Gunrock configuration assumes boost
(1.58.0 or 1.59.0) and metis are installed; if not, changes need to be made in
the Makefiles. DARPA's DGX-1 has both installed when the tests are performed.

```shell
git clone --recursive https://github.com/gunrock/gunrock/
cd gunrock
git checkout dev-refactor
git submodule init
git submodule update
mkdir build
cd build
cmake ..
cd ../tests/gtf
make
```
At this point, there should be an executable `gtf_main_<CUDA version>_x86_64`
in `tests/gtf/bin`.

The testing is done with Gunrock using `dev-refactor` branch at commit `2699252`
(Oct. 18, 2018), using CUDA 9.1 with NVIDIA driver 390.30.

### HIVE Data Preparation

Prepare the data; skip this step if you are just running the sample dataset.

Refer to `parse_args()` in `taxi_tsv_file_preprocessing.py` for dataset preprocessing options.
Set the `lambda1` (see equation above) in generate_graph.py for Gunrock.

```shell
cd gunrock/tests/gtf/_data

export TOKEN= # get this Authentication TOKEN from https://api-token.hiveprogram.com/#!/user
mkdir -p _data
wget --header "Authorization:$TOKEN" \
  https://hiveprogram.com/data/_v0/sparse_fused_lasso/taxi/taxi-small.tar.gz
tar -xzvf taxi-small.tar.gz && rm -r taxi-small.tar.gz
mv taxi-small _data/

wget --header "Authorization:$TOKEN" \
  https://hiveprogram.com/data/_v0/sparse_fused_lasso/taxi/taxi-1M.tar.gz
tar -xzvf taxi-1M.tar.gz && rm -r taxi-1M.tar.gz
mv taxi-1M _data/

python taxi_tsv_file_preprocessing.py
python generate_graph.py
```

Three files are generated. The files `e` and `n` are for benchmarks, and `std_added.mtx` is for Gunrock input.

### Running the application
```
--lambda2 is the sparsity regularization strength
```
Sample command line with argument.
```shell
./bin/test_gtf_10.0_x86_64 market ./_data/std_added.mtx --lambda2 3
```

### Output

The code will output two files in the current directory. One is called `output_pr.txt` (for CPU reference) and the other is called `output_pr_GPU.txt` (for GPU SFL with push-relabel backend).
Each vertex's new weight will be stored in each line of the two files. These outputs could be further processed into the resulting heatmap.
The printout after running `gtf_main_<CUDA version>_x86_64` includes the timing of the application.

Sample `txt` output:

```
0
0
0
0
0
0
-11.375
-0.307292
0
```

Sample output on console:

```
______CPU reference algorithm______                                                                                                                                  [9/1944]
offset is 58522 num edges 76366
!!!!!!!!!! avg is -0.876037
Iteration 0
Iteration 1
Iteration 2
Iteration 3
Iteration 4
Iteration 5
Iteration 6
Iteration 7
Iteration 8
Iteration 9
Iteration 10
Iteration 11
-----------------------------------
Elapsed: 5500.730991 ms
in side that weird function1
offset in GPU preprocess is 58522 num edges 76366
avg in GPU preprocess is -0.876037
Using advance mode LB
Using filter mode CULL
Using advance mode LB
Using filter mode CULL
offset is 58522 num edges 76366
h_community_accus is -0.876037
______GPU SFL algorithm____
enact calling successfully!!!!!!!!!!!
-----------------------------------
Run 0, elapsed: 3341.358185 ms, #iterations = 12
transfering to host!!!: 8924
[gtf] finished.
 avg. elapsed: 3341.358185 ms
 iterations: 0
 min. elapsed: 3341.358185 ms
 max. elapsed: 3341.358185 ms
 load time: 138.952 ms
 preprocess time: 342.184000 ms
 postprocess time: 0.258923 ms
 total time: 3845.412016 ms

```

## Performance and Analysis

We measure the runtime and loss function `0.5 * sum((y' - y)^2) + lambda1 * sum(abs(y_i' - y_j')) + lambda2 * sum(abs(yi'))`, where `y` is the input weight (observation) for each vertex and `y'` (fitted weight) is the new weight for each vertex.

### Implementation limitations

The time is mostly spent on maxflow computation. Each iteration of the SFL calls a maxflow. For a 8922-vertex, 20349-edge dataset, the time spent on maxflow vs. the rest of the SFL post-processing is around 20:1 per iteration. The maxflow implementation has room for further optimization; we expect to have shorter runtimes on maxflow in the future. We only implement serial SFL renormalization (second part) for correctness purposes. If the graph is larger, we expect the ratio between maxflow (first part) and SFL renormalization (second part) will be lower, because the runtime of the renormalization is serial.

### Comparison against existing implementations
Graphtv is an official implementation of sparse fused lasso algorithm with a parametric maxflow backend. It is a CPU serial implementation <https://www.cs.ucsb.edu/~yuxiangw/codes/gtf_code.zip>. The Gunrock GPU runtime is measured between the application enactor and it is an output of the application.

| DataSet | time starts | time ends | #E | #V | graphtv runtime | Gunrock GPU runtime |
|-------------- |---------------------|--------------------|----------|----------|------| ---|
| NY Taxi-small | 2011-06-26 12:00:00 |2011-06-26 14:00:00 | 20349 | 8922 | 0.11s |  *3.23s |
| NY Taxi-small | 2011-06-26 00:00:00 |2011-06-27 00:00:00 | 293259 | 107064 | 8.71s | |
| NY Taxi-1M | 2011-06-19 00:00:00 |2011-06-27 00:00:00 | 588211 | 213360 | 103.62s |  |

| DataSet | time starts | time ends | #E | #V | graphtv loss | Gunrock GPU loss |
|-------------- |---------------------|--------------------|----------|----------|-------------------| -------------------|
| NY Taxi-small | 2011-06-26 12:00:00 |2011-06-26 14:00:00 | 20349 | 8922 | 132789.32 | *132789.32 |
| NY Taxi-small | 2011-06-26 00:00:00 |2011-06-27 00:00:00 | 293259 | 107064 | |                     |
| NY Taxi-1M | 2011-06-19 00:00:00 |2011-06-27 00:00:00 | 588211 | 213360 | |                     |

`*`: The GPU implementation of maxflow still has a bug that leads to different results per run. We only report the best result we can achieve with the current version of maxflow. Since this current version of maxflow is able to achieve the same loss as Graphtv, we presume that the results, both the runtime and loss, should be similar or even better with a bug-free maxflow.

### Performance limitations

We observe a slowdown when we compare Graphtv and Gunrock's current SFL implementation on the `NY Taxi-small` dataset with time from `2011-06-26 12:00:00` to `2011-06-26 14:00:00` for the following reasons:

1. It seems we are only running it on a small graph successfully (~9000 nodes), which is not enough to saturate the GPU at all. This only applies to this dataset.
2. The bottleneck is still the MF implementation.
    1. A preflow computation on the CPU in the problem.reset() requires a for-loop of `src.edges` and a Move operation (very costly) from GPU to CPU every time MF is called.
    2. Its relabeling heuristics are also on the CPU, so in every iteration it needs to move the flow and heights back to CPU and do this relabeling for a certain number of iterations.
    3. There are device syncs calls between all of these operations, as well as slow, global barriers.
3. We also have iteration boundaries right now (like BC) to perform MinCut on the GPU.
4. The SLF renormalization is serial.

We expect the above problems to be solved with future implementations of Maxflow.

## Next Steps

### Alternate approaches(Done)

For CPU, the parametric maxflow algorithm works well, but it is not parallelizable to GPU. The push-relabel algorithm we have on Gunrock's maxflow should be the best implementation among the parallelizable algorithms on GPU.

### Gunrock implications

SFL is the first algorithm that stacks previous applications. Some data pre-processing that is common to execute in the CPU requires better designs of the APIs which will facilitate new applications. For example, `gtf_enactor` needs to call `mf_problem.reset()`. Since the current maxflow code does not have any preprocessing of the graph on GPU, SFL has to transfer the data back and forth between CPU and GPU and SRL requires unnecessary arrays to store the Maxflow input arrays. The preferred implementation does maxflow preprocessing on the GPU.

Moreover, a unit test framework would be very helpful for development. If we don't do unit tests on important functions, it is hard to track the problems and discover simple, avoidable but missing test cases, such as comparison between two double values in SFL. A mockito test framework may be appropriate. http://www.vogella.com/tutorials/Mockito/article.html

### Notes on multi-GPU parallelization

> What will be the challenges in parallelizing this to multiple GPUs on the same node?
>
> Can the dataset be effectively divided across multiple GPUs, or must it be replicated?

### Notes on dynamic graphs

(Only if appropriate)

> Does this workload have a dynamic-graph component? If so, what are the implications of that? How would your implementation change? What support would Gunrock need to add?

### Notes on larger datasets

> What if the dataset was larger than can fit into GPU memory or the aggregate GPU memory of multiple GPUs on a node? What implications would that have on performance? What support would Gunrock need to add?

### Notes on other pieces of this workload

> Briefly: What are the important other (non-graph) pieces of this workload? Any thoughts on how we might implement them / what existing approaches/libraries might implement them?

### Research opportunities

> "{what you've done with respect to this workflow / what you could do} that you think has research value and could lead to a paper". We should write those down now when they're fresh in our minds. This is less for DARPA and more for us, for our discussion. I know not every one of these workflows will lead to a research advance. But there are research advances in implementing the workflow AND implementing interesting parts of the workflow (e.g., sparse data structures) AND other things we could use pieces of the workflow to solve (e.g., auction algorithm)

Prof. Sharpnack indicates that this implementation could be generalized to multi-graph fused lasso. The idea is to set multiple edge values for the edges connecting to source/sink, while keeping the graph topology and edge values (lambda1) for the edges in the original graph (excluding source and sink) the same.
